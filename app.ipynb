{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from langdetect import detect_langs \n",
    "from label import get_label #this function can get the emotion label of an input\n",
    "from rank_bm25 import BM25Okapi,BM25L, BM25Plus #this package can calculate BM25 for each document\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize #this package tokenize word by whitespace and punctuation\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from metrics import MAP, NDCG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('cleaned_query.csv').drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df=pd.read_csv('data/emotionsong.csv').drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# song_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fliter out no lyrics songs\n",
    "song_df=song_df[song_df['lyrics']!='no lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is to check some examples of lyrics\n",
    "# print(song_df['lyrics'].iloc[3])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove round brackets and curly brackets but not text within\n",
    "song_df['lyrics'] = song_df['lyrics'].map(lambda s: re.sub(r'\\(|\\)', '', s))\n",
    "song_df['lyrics'] = song_df['lyrics'].map(lambda s: re.sub(r'\\{|\\}', '', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Line Breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove line breaks\n",
    "song_df['lyrics'] = song_df['lyrics'].map(lambda s: re.sub(r' \\n|\\n', '', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove non-English Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english songs: 2937\n",
      "Number of non-english songs: 340\n"
     ]
    }
   ],
   "source": [
    "def get_eng_prob(text):\n",
    "    detections = detect_langs(text)\n",
    "    for detection in detections:\n",
    "        if detection.lang == 'en':\n",
    "            return detection.prob\n",
    "    return 0\n",
    "\n",
    "song_df['en_prob'] = song_df['lyrics'].map(get_eng_prob)\n",
    "\n",
    "print('Number of english songs: {}'.format(sum(song_df['en_prob'] >= 0.5)))\n",
    "print('Number of non-english songs: {}'.format(sum(song_df['en_prob'] < 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = song_df.loc[song_df['en_prob'] >= 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_url(file):\n",
    "    file_lowered=file.lower() \n",
    "    file_url=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', file_lowered, flags=re.MULTILINE) #remove url\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', file_url)\n",
    "# lower vocab and remove url and emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df['lyrics'] = song_df['lyrics'].map(lower_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dig(tokens):\n",
    "    #remove punctuations & numbers\n",
    "    filtered_words = [re.sub(r'\\d+', '', word) for word in tokens]\n",
    "    return filtered_words\n",
    "# remove digits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df['tokens']= song_df['tokens'].map(remove_dig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(tokens):\n",
    "    f = open(\"stoplist.txt\", \"r\")\n",
    "    stoplist=f.read()\n",
    "    words = [w for w in tokens if not w in stoplist]\n",
    "    return words\n",
    "# remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stoplist.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a372d19362a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msong_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msong_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_stop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3628\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3629\u001b[0m         \"\"\"\n\u001b[1;32m-> 3630\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3631\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1145\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-3b29e3105d79>\u001b[0m in \u001b[0;36mremove_stop\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stoplist.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mstoplist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstoplist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stoplist.txt'"
     ]
    }
   ],
   "source": [
    "song_df['tokens']= song_df['tokens'].map(remove_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "def get_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "def lemma_token(tokens):\n",
    "    tagged_words=nltk.pos_tag(tokens)\n",
    "    new_token = []\n",
    "    for i in tagged_words:\n",
    "        wordnet_pos = get_pos(i[1]) #or wordnet.NOUN\n",
    "        new_token.append(wnl.lemmatize(i[0],pos=wordnet_pos))\n",
    "    return new_token\n",
    "# lemmetize my bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df['tokens']= song_df['tokens'].map(lemma_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## get the code for all songs in corpus\n",
    "# codes=q_df['song_id']\n",
    "# codes=codes.values.tolist()\n",
    "\n",
    "# ## using codes to get lyrics for songs to create corpus for ranking search\n",
    "# corpus=[song_df[song_df['code']==i].lyrics.to_string(index=False) for i in codes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion=[]\n",
    "for i in labels:\n",
    "    if i=='0':\n",
    "        emotion.append('negative')\n",
    "    elif i=='1':\n",
    "        emotion.append('positive')\n",
    "    else:\n",
    "        emotion.append('neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries.sort()\n",
    "labels=['0','1','2','1','0','1','0','1','2','2','1','0','1','0','1','0','0','0','1','2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE LSI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages for gensim\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Corpus for All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(test_response,query):\n",
    "#     texts_doc = [\n",
    "#       [word for word in document.lower().split() if word not in stoplist]\n",
    "#       for document in test_response]\n",
    "\n",
    "# remove words that appear only once\n",
    "    frequency_test = defaultdict(int)\n",
    "    for text in test_response:\n",
    "        for token in text:\n",
    "            frequency_test[token] += 1\n",
    "\n",
    "    texts_doc = [\n",
    "        [token for token in text if frequency_test[token] > 1]\n",
    "        for text in test_response\n",
    "        ]\n",
    "    dictionary = corpora.Dictionary(texts_doc)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts_doc]\n",
    "    lsi_new = models.LsiModel(corpus, id2word=dictionary, num_topics=3)\n",
    "    index = similarities.MatrixSimilarity(lsi_new[corpus])  \n",
    "    vec_bow = dictionary.doc2bow(query.lower().split())\n",
    "    vec_lsi = lsi_new[vec_bow]  # convert the query to LSI space\n",
    "    sims_1 = index[vec_lsi] \n",
    "#     newsim = sorted(enumerate(sims_1), key=lambda item: -item[1])\n",
    "   \n",
    "    return sims_1 # get  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_lsi(query):\n",
    "    song_copy=song_df.copy()\n",
    "    label=get_label(query)\n",
    "    #filter out corresponding songs\n",
    "    if label=='0':\n",
    "        corpus_df=song_copy[song_copy['final_score']=='negative'].reset_index(drop=True)\n",
    "    elif label=='1':\n",
    "        corpus_df=song_copy[song_copy['final_score']=='positive'].reset_index(drop=True)\n",
    "    else:\n",
    "        corpus_df=song_copy[song_copy['final_score']=='neutral'].reset_index(drop=True)\n",
    "    corpus=corpus_df['tokens'].values\n",
    "    doc_scores = get_corpus(corpus,query)\n",
    "    corpus_df['rank_score']=doc_scores\n",
    "    song_info=corpus_df.sort_values(by='rank_score',ascending=False)[0:10]\n",
    "    return song_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_songs():\n",
    "    query=input(\"Enter your emotion: \")\n",
    "    song_info=get_song_lsi(query)\n",
    "    for index, row in song_info.iterrows():\n",
    "        print(\"Song name:\"+row['song'],',', \"Singer Name:\" + row['singer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# song_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your emotion: we are having a meeting now\n",
      "Song name:Angel in the Snow , Singer Name:Elliott Smith\n",
      "Song name:One Moment Please... , Singer Name:Kenny Larkin\n",
      "Song name:Falling Man , Singer Name:Blonde Redhead\n",
      "Song name:Could I Be You (Album Version) , Singer Name:Matchbox Twenty\n",
      "Song name:The Absence Of Your Company , Singer Name:Kim Richey\n",
      "Song name:Dreaming , Singer Name:Loudon Wainwright III\n",
      "Song name:I Don't Have Anything , Singer Name:VAST\n",
      "Song name:Who Am I , Singer Name:Will Young\n",
      "Song name:Never Loved A Girl , Singer Name:Aerosmith\n",
      "Song name:The Debt Collectors , Singer Name:Ben Lee\n"
     ]
    }
   ],
   "source": [
    "print_songs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
