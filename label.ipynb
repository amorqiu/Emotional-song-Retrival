{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# display all rows and columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('max_colwidth',1000)\n",
    "# filter warningsb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate query dataframe\n",
    "with open('friends.json') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    for record in data[i]:\n",
    "        series = pd.Series({\"sentence\":record['utterance'],\"emotion\":record['emotion']})\n",
    "        df = df.append(series, ignore_index=True)\n",
    "# choose queries more than 5 words, which provide more information\n",
    "df2=df[df['sentence'].apply(lambda x: len(str(x).split())>=5)]\n",
    "df2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query  = df2[df2.index.isin([66,157,160,189,149,380,386,32,91,155,176,272,659,25,127,152,174,1303,2184,7419])]\n",
    "query  = df2[df2.index.isin([174,176,189,272,380,386,659,1301,3509,7419,25,32,66,91,127,149,152,155,157,160])]\n",
    "# 174,176,189,272,380,386,659,1301,3509,7419,25,32,66,91,127,149,152,155,157,160\n",
    "# 1301,3509,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre- processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Or! Or, we could go to the bank, close our accounts and cut them off at the source.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Aww, man, now we wont be bank buddies!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Ohh, you guys, remember that cute client I told you about? I bit him.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Well, next time your massaging him, you should try and distract yourself.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>joy</td>\n",
       "      <td>Yeah! Yeah! Yeah! Like-like when Im doing something exciting and I dont wanna get</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion  \\\n",
       "1  neutral   \n",
       "2  sadness   \n",
       "3  neutral   \n",
       "5  neutral   \n",
       "6      joy   \n",
       "\n",
       "                                                                              sentence  \\\n",
       "1  Or! Or, we could go to the bank, close our accounts and cut them off at the source.   \n",
       "2                                              Aww, man, now we wont be bank buddies!   \n",
       "3                Ohh, you guys, remember that cute client I told you about? I bit him.   \n",
       "5            Well, next time your massaging him, you should try and distract yourself.   \n",
       "6  Yeah! Yeah! Yeah! Like-like when Im doing something exciting and I dont wanna get   \n",
       "\n",
       "   sentiment_id  \n",
       "1             2  \n",
       "2             0  \n",
       "3             2  \n",
       "5             2  \n",
       "6             1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classify emotions into 3 categories\n",
    "df2=df2[df2['emotion']!='non-neutral']\n",
    "sent_to_id  = {\"anger\":0,'sadness':0,'disgust':0,'fear':0,'surprise':1, \"joy\":1,\"neutral\":2}\n",
    "df2[\"sentiment_id\"] = df2['emotion'].map(sent_to_id)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "df2['cleaned_sentence'] = df2['sentence'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pacakges\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7100 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# split train/test dataset\n",
    "train, test = train_test_split(df2, test_size=0.000001 , random_state=42)\n",
    "# tokenize the words\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) <= 0:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "# tagged datasets\n",
    "train_tagged = train.apply(lambda x: TaggedDocument(words=tokenize_text(x['cleaned_sentence']), tags=[x.sentiment_id]), axis=1)\n",
    "test_tagged = test.apply(lambda x: TaggedDocument(words=tokenize_text(x['cleaned_sentence']), tags=[x.sentiment_id]), axis=1)\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "max_fatures = 500000\n",
    "\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "#tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ', filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df2['cleaned_sentence'].values)\n",
    "X = tokenizer.texts_to_sequences(df2['cleaned_sentence'].values)\n",
    "X = pad_sequences(X,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Found %s unique tokens.' % len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build doc2vev model for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7099/7099 [00:00<00:00, 2372538.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/m,d20,n5,w8,s0.001)\n"
     ]
    }
   ],
   "source": [
    "d2v_model = Doc2Vec(dm=1, dm_mean=1, size=20, window=8, min_count=1, workers=1, alpha=0.065, min_alpha=0.065)\n",
    "d2v_model.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "print(d2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vectors in a new matrix\n",
    "embedding_matrix = np.zeros((len(d2v_model.wv.vocab)+ 1, 20))\n",
    "for i, vec in enumerate(d2v_model.docvecs.vectors_docs):\n",
    "    while i in vec <= 1000:\n",
    "          embedding_matrix[i]=vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pacakges\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 20)            120080    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                14200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 134,433\n",
      "Trainable params: 134,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## initiate the model\n",
    "# init layer\n",
    "model = Sequential()\n",
    "\n",
    "# emmbed word vectors\n",
    "model.add(Embedding(len(d2v_model.wv.vocab)+1,20,input_length=X.shape[1],weights=[embedding_matrix],trainable=True))\n",
    "\n",
    "# learn the correlations\n",
    "def split_input(sequence):\n",
    "     return sequence[:-1], tf.reshape(sequence[1:], (-1,1))\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "model.add(Dense(3,activation=\"softmax\"))\n",
    "\n",
    "# output model skeleton\n",
    "model.summary()\n",
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6035, 50) (6035, 3)\n",
      "(1065, 50) (1065, 3)\n"
     ]
    }
   ],
   "source": [
    "# split train/test dataset for LSTM model\n",
    "Y = pd.get_dummies(df2['sentiment_id']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "189/189 - 2s - loss: 0.5819 - acc: 0.5508\n",
      "Epoch 2/50\n",
      "189/189 - 2s - loss: 0.5211 - acc: 0.6013\n",
      "Epoch 3/50\n",
      "189/189 - 2s - loss: 0.4456 - acc: 0.6590\n",
      "Epoch 4/50\n",
      "189/189 - 2s - loss: 0.3792 - acc: 0.7312\n",
      "Epoch 5/50\n",
      "189/189 - 2s - loss: 0.3068 - acc: 0.7978\n",
      "Epoch 6/50\n",
      "189/189 - 2s - loss: 0.2445 - acc: 0.8469\n",
      "Epoch 7/50\n",
      "189/189 - 2s - loss: 0.2026 - acc: 0.8766\n",
      "Epoch 8/50\n",
      "189/189 - 2s - loss: 0.1787 - acc: 0.8915\n",
      "Epoch 9/50\n",
      "189/189 - 2s - loss: 0.1574 - acc: 0.9080\n",
      "Epoch 10/50\n",
      "189/189 - 2s - loss: 0.1423 - acc: 0.9158\n",
      "Epoch 11/50\n",
      "189/189 - 2s - loss: 0.1266 - acc: 0.9279\n",
      "Epoch 12/50\n",
      "189/189 - 2s - loss: 0.1195 - acc: 0.9304\n",
      "Epoch 13/50\n",
      "189/189 - 2s - loss: 0.1076 - acc: 0.9364\n",
      "Epoch 14/50\n",
      "189/189 - 2s - loss: 0.1018 - acc: 0.9408\n",
      "Epoch 15/50\n",
      "189/189 - 2s - loss: 0.0951 - acc: 0.9422\n",
      "Epoch 16/50\n",
      "189/189 - 2s - loss: 0.0857 - acc: 0.9501\n",
      "Epoch 17/50\n",
      "189/189 - 2s - loss: 0.0855 - acc: 0.9490\n",
      "Epoch 18/50\n",
      "189/189 - 2s - loss: 0.0785 - acc: 0.9553\n",
      "Epoch 19/50\n",
      "189/189 - 2s - loss: 0.0753 - acc: 0.9556\n",
      "Epoch 20/50\n",
      "189/189 - 2s - loss: 0.0687 - acc: 0.9584\n",
      "Epoch 21/50\n",
      "189/189 - 2s - loss: 0.0716 - acc: 0.9587\n",
      "Epoch 22/50\n",
      "189/189 - 2s - loss: 0.0629 - acc: 0.9626\n",
      "Epoch 23/50\n",
      "189/189 - 2s - loss: 0.0581 - acc: 0.9675\n",
      "Epoch 24/50\n",
      "189/189 - 2s - loss: 0.0600 - acc: 0.9644\n",
      "Epoch 25/50\n",
      "189/189 - 2s - loss: 0.0561 - acc: 0.9690\n",
      "Epoch 26/50\n",
      "189/189 - 2s - loss: 0.0527 - acc: 0.9703\n",
      "Epoch 27/50\n",
      "189/189 - 2s - loss: 0.0561 - acc: 0.9690\n",
      "Epoch 28/50\n",
      "189/189 - 2s - loss: 0.0516 - acc: 0.9692\n",
      "Epoch 29/50\n",
      "189/189 - 2s - loss: 0.0460 - acc: 0.9760\n",
      "Epoch 30/50\n",
      "189/189 - 2s - loss: 0.0512 - acc: 0.9698\n",
      "Epoch 31/50\n",
      "189/189 - 2s - loss: 0.0502 - acc: 0.9722\n",
      "Epoch 32/50\n",
      "189/189 - 2s - loss: 0.0434 - acc: 0.9760\n",
      "Epoch 33/50\n",
      "189/189 - 2s - loss: 0.0400 - acc: 0.9786\n",
      "Epoch 34/50\n",
      "189/189 - 2s - loss: 0.0365 - acc: 0.9803\n",
      "Epoch 35/50\n",
      "189/189 - 2s - loss: 0.0376 - acc: 0.9785\n",
      "Epoch 36/50\n",
      "189/189 - 2s - loss: 0.0348 - acc: 0.9818\n",
      "Epoch 37/50\n",
      "189/189 - 2s - loss: 0.0331 - acc: 0.9814\n",
      "Epoch 38/50\n",
      "189/189 - 2s - loss: 0.0357 - acc: 0.9800\n",
      "Epoch 39/50\n",
      "189/189 - 2s - loss: 0.0315 - acc: 0.9824\n",
      "Epoch 40/50\n",
      "189/189 - 2s - loss: 0.0270 - acc: 0.9856\n",
      "Epoch 41/50\n",
      "189/189 - 2s - loss: 0.0305 - acc: 0.9828\n",
      "Epoch 42/50\n",
      "189/189 - 2s - loss: 0.0434 - acc: 0.9753\n",
      "Epoch 43/50\n",
      "189/189 - 2s - loss: 0.0342 - acc: 0.9791\n",
      "Epoch 44/50\n",
      "189/189 - 2s - loss: 0.0303 - acc: 0.9843\n",
      "Epoch 45/50\n",
      "189/189 - 2s - loss: 0.0320 - acc: 0.9821\n",
      "Epoch 46/50\n",
      "189/189 - 2s - loss: 0.0299 - acc: 0.9836\n",
      "Epoch 47/50\n",
      "189/189 - 2s - loss: 0.0219 - acc: 0.9876\n",
      "Epoch 48/50\n",
      "189/189 - 2s - loss: 0.0196 - acc: 0.9892\n",
      "Epoch 49/50\n",
      "189/189 - 2s - loss: 0.0193 - acc: 0.9891\n",
      "Epoch 50/50\n",
      "189/189 - 2s - loss: 0.0331 - acc: 0.9834\n"
     ]
    }
   ],
   "source": [
    "# model fit\n",
    "batch_size = 32\n",
    "history=model.fit(X_train, Y_train, epochs =50, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 - 1s - loss: 0.0223 - acc: 0.9881\n",
      "34/34 - 0s - loss: 2.5057 - acc: 0.5296\n",
      "Train: 0.988, Test: 0.5296\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, Y_train, verbose=2)\n",
    "_, test_acc = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print('Train: %.3f, Test: %.4f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 4ms/step - loss: 2.3264 - acc: 0.5540\n",
      "score: 2.33\n",
      "acc: 0.55\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "validation_size = 500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = batch_size)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get query emotion\n",
    "def get_label(x):\n",
    "    seq = tokenizer.texts_to_sequences([x])\n",
    "    padded = pad_sequences(seq, maxlen=50, dtype='int32', value=0)\n",
    "    pred = model.predict(padded)\n",
    "    labels = ['0','1','2']\n",
    "    return labels[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>sentence</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>anger</td>\n",
       "      <td>You had no right to tell me you ever had feelings for me.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>joy</td>\n",
       "      <td>Oh, it's so romantic to send people off on their honeymoon.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>neutral</td>\n",
       "      <td>This witness won't return my calls so we're gonna see if we can surprise him coming home.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>joy</td>\n",
       "      <td>Okay, okay, come on, you can do it. You can do it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>anger</td>\n",
       "      <td>My fault?! You threatened the judge!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Everyone knows who you are.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I mean, well, 'cause when I was growing up, you know my dad left, and my mother died, and my stepfather went to jail, so I barely had enough pieces of parents to make one whole one.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>joy</td>\n",
       "      <td>It's just, it's just the luckiest baby in the whole world.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Okay, it's a typical New York City apartment. Two girls are just hanging out.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>neutral</td>\n",
       "      <td>It's my new perfume. Why don't you come closer where you can</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>sadness</td>\n",
       "      <td>My marriage, I think my marriage is um, is kinda over.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Oh my God! I dont believe it! Oh, you poor bunny.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Look, when it started I was just trying to be nice to her because she was my brother's girlfriend.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Check it out, hes winning!  Petes winning!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I got a call at two in the morning, but all I could hear was, like, this high squeaky sound, so I thought okay its like a mouse or a opossum.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Okay, okay, but dont worry, because we also have cereals, muffins, waffles,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Wait a minute, the house was built on radioactive waste, and an ancient Indian burial ground? That would never happen.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Here. I need to borrow some moisturizer.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>fear</td>\n",
       "      <td>You know you probably didn't know this, but back in high school, I had a, um, major crush on you.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7419</th>\n",
       "      <td>fear</td>\n",
       "      <td>But only because I was up all night worried about this  meeting, ain’t that funny?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       emotion  \\\n",
       "25       anger   \n",
       "32         joy   \n",
       "66     neutral   \n",
       "91         joy   \n",
       "127      anger   \n",
       "149    neutral   \n",
       "152    sadness   \n",
       "155        joy   \n",
       "157    neutral   \n",
       "160    neutral   \n",
       "174    sadness   \n",
       "176   surprise   \n",
       "189    neutral   \n",
       "272   surprise   \n",
       "380    neutral   \n",
       "386    neutral   \n",
       "659   surprise   \n",
       "1301   neutral   \n",
       "3509      fear   \n",
       "7419      fear   \n",
       "\n",
       "                                                                                                                                                                                   sentence  \\\n",
       "25                                                                                                                                You had no right to tell me you ever had feelings for me.   \n",
       "32                                                                                                                              Oh, it's so romantic to send people off on their honeymoon.   \n",
       "66                                                                                                This witness won't return my calls so we're gonna see if we can surprise him coming home.   \n",
       "91                                                                                                                                       Okay, okay, come on, you can do it. You can do it!   \n",
       "127                                                                                                                                                    My fault?! You threatened the judge!   \n",
       "149                                                                                                                                                             Everyone knows who you are.   \n",
       "152   I mean, well, 'cause when I was growing up, you know my dad left, and my mother died, and my stepfather went to jail, so I barely had enough pieces of parents to make one whole one.   \n",
       "155                                                                                                                              It's just, it's just the luckiest baby in the whole world.   \n",
       "157                                                                                                           Okay, it's a typical New York City apartment. Two girls are just hanging out.   \n",
       "160                                                                                                                            It's my new perfume. Why don't you come closer where you can   \n",
       "174                                                                                                                                  My marriage, I think my marriage is um, is kinda over.   \n",
       "176                                                                                                                                      Oh my God! I dont believe it! Oh, you poor bunny.   \n",
       "189                                                                                      Look, when it started I was just trying to be nice to her because she was my brother's girlfriend.   \n",
       "272                                                                                                                                            Check it out, hes winning!  Petes winning!   \n",
       "380                                           I got a call at two in the morning, but all I could hear was, like, this high squeaky sound, so I thought okay its like a mouse or a opossum.   \n",
       "386                                                                                                            Okay, okay, but dont worry, because we also have cereals, muffins, waffles,   \n",
       "659                                                                  Wait a minute, the house was built on radioactive waste, and an ancient Indian burial ground? That would never happen.   \n",
       "1301                                                                                                                                               Here. I need to borrow some moisturizer.   \n",
       "3509                                                                                      You know you probably didn't know this, but back in high school, I had a, um, major crush on you.   \n",
       "7419                                                                                                     But only because I was up all night worried about this  meeting, ain’t that funny?   \n",
       "\n",
       "     new  \n",
       "25     0  \n",
       "32     1  \n",
       "66     2  \n",
       "91     1  \n",
       "127    0  \n",
       "149    2  \n",
       "152    0  \n",
       "155    1  \n",
       "157    2  \n",
       "160    2  \n",
       "174    0  \n",
       "176    1  \n",
       "189    2  \n",
       "272    1  \n",
       "380    2  \n",
       "386    2  \n",
       "659    1  \n",
       "1301   2  \n",
       "3509   0  \n",
       "7419   0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking\n",
    "query['new']=  query['sentence'].apply(lambda x: get_label(x))\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### if using the function. e.g+\n",
    "message='''it's just, it's just the luckiest baby in the whole world.'''\n",
    "emotion_category=get_label(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
